{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generator_verse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzDuFx2Zaq0D"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import time\r\n",
        "import unicodedata\r\n",
        "import re\r\n",
        "import io\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBNH0oJma7Ce",
        "outputId": "82906d65-cc9c-41a8-c92d-a119480e2972"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive');"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQde89Z4dszF"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/Colab Notebooks/GeekBrains/NLP/Project/comedy.txt'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSDHXizTd33j",
        "outputId": "3c501621-4708-4b02-bc6f-da206f824731"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='1251')\r\n",
        "# length of text is the number of characters in it\r\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 808629 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E42SqKn1d9b7",
        "outputId": "8aeb0205-be0b-4c7d-ee95-b6be4c3d6987"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * АД * \r\n",
            "\r\n",
            "\r\n",
            "ПЕСНЬ ПЕРВАЯ\r\n",
            "\r\n",
            "\r\n",
            "                    1 Земную жизнь пройдя до половины,\r\n",
            "                      Я очутился в сумрачном лесу,\r\n",
            "                      Утратив правый путь во тьме долины.\r\n",
            "\r\n",
            "                    4 Каков он был, о, как произнесу,\r\n",
            "                      Тот дикий лес, дремучий и грозящий,\r\n",
            "                      Чей давний ужас в памяти несу!\r\n",
            "\r\n",
            "                    7 Так горек он, что смерть едва ль не слаще.\r\n",
            "                      Но, благо в нем обретши навсегда,\r\n",
            "      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3joGzukeA_m",
        "outputId": "ed7d374b-ee79-4ffb-a9f9-dfa09704267e"
      },
      "source": [
        "# The unique characters in the file\r\n",
        "vocab = sorted(set(text))\r\n",
        "print('{} unique words'.format(len(vocab)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "130 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RXrm7_bgTBF"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\r\n",
        "idx2char = np.array(vocab)\r\n",
        "\r\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcO3_L_YgoPS"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\r\n",
        "seq_length = 100\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "\r\n",
        "# Create training examples / targets\r\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SE8-tcvhrLp"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dx1i0DkinJM"
      },
      "source": [
        "def split_input_target(chunk):\r\n",
        "    input_text = chunk[:-1]\r\n",
        "    target_text = chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy_NHdsGjEIj",
        "outputId": "f15e5344-3368-4870-c0e4-023acfe4de1c"
      },
      "source": [
        "# Batch size\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "\r\n",
        "dataset"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzIvqxUqjQEf"
      },
      "source": [
        "# Length of the vocabulary in chars\r\n",
        "vocab_size = len(vocab)\r\n",
        "\r\n",
        "# The embedding dimension\r\n",
        "embedding_dim = 256\r\n",
        "\r\n",
        "# Number of RNN units\r\n",
        "rnn_units = 512"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWSckrbpjW8q"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n",
        "                                  batch_input_shape=[batch_size, None]),\r\n",
        "                                 \r\n",
        "        tf.keras.layers.GRU(rnn_units,\r\n",
        "                            return_sequences=True,\r\n",
        "                            stateful=True,\r\n",
        "                            recurrent_initializer='glorot_uniform'),\r\n",
        "\r\n",
        "        tf.keras.layers.GRU(rnn_units,\r\n",
        "                            return_sequences=True,\r\n",
        "                            stateful=True,\r\n",
        "                            recurrent_initializer='glorot_uniform'),\r\n",
        "\r\n",
        "         tf.keras.layers.GRU(rnn_units,\r\n",
        "                            return_sequences=True,\r\n",
        "                            stateful=True,\r\n",
        "                            recurrent_initializer='glorot_uniform'),\r\n",
        "                                   \r\n",
        "        tf.keras.layers.Dense(vocab_size)\r\n",
        "    ])\r\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv9jRLdHjaoe"
      },
      "source": [
        "model = build_model(\r\n",
        "    vocab_size=len(vocab),\r\n",
        "    embedding_dim=embedding_dim,\r\n",
        "    rnn_units=rnn_units,\r\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CBCNDz1jarI",
        "outputId": "22e6e2bb-68aa-4452-c3e5-9320f7665e55"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_example_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 130) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTGIbSR5jat0",
        "outputId": "411e56ea-bae4-4c25-91da-38de80e0561f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           33280     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 512)           1182720   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 512)           1575936   \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (64, None, 512)           1575936   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 130)           66690     \n",
            "=================================================================\n",
            "Total params: 4,434,562\n",
            "Trainable params: 4,434,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpm7h4ThjawO"
      },
      "source": [
        "def loss(labels, logits):\r\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhhZO0__y9jf"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pycByhBSjays"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQbYo1grlKpb"
      },
      "source": [
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = './checkpoints'\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_freq=800,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF2RHPHclKsR"
      },
      "source": [
        "EPOCHS = 500"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HSlzZ5BlKuw",
        "outputId": "782f7301-bb6f-4dcd-b840-8c6b6c5f340f"
      },
      "source": [
        "history = model.fit(\r\n",
        "    dataset,\r\n",
        "    epochs=EPOCHS,\r\n",
        "    callbacks=[checkpoint_callback, early_stopping])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "125/125 [==============================] - 12s 70ms/step - loss: 2.6453\n",
            "Epoch 2/500\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.6347\n",
            "Epoch 3/500\n",
            "125/125 [==============================] - 10s 71ms/step - loss: 1.4469\n",
            "Epoch 4/500\n",
            "125/125 [==============================] - 10s 71ms/step - loss: 1.3203\n",
            "Epoch 5/500\n",
            "125/125 [==============================] - 10s 71ms/step - loss: 1.2314\n",
            "Epoch 6/500\n",
            "125/125 [==============================] - 10s 71ms/step - loss: 1.1738\n",
            "Epoch 7/500\n",
            "125/125 [==============================] - 10s 72ms/step - loss: 1.1311\n",
            "Epoch 8/500\n",
            "125/125 [==============================] - 10s 72ms/step - loss: 1.0941\n",
            "Epoch 9/500\n",
            "125/125 [==============================] - 10s 72ms/step - loss: 1.0613\n",
            "Epoch 10/500\n",
            "125/125 [==============================] - 10s 72ms/step - loss: 1.0347\n",
            "Epoch 11/500\n",
            "125/125 [==============================] - 10s 72ms/step - loss: 1.0137\n",
            "Epoch 12/500\n",
            "125/125 [==============================] - 10s 73ms/step - loss: 0.9904\n",
            "Epoch 13/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.9680\n",
            "Epoch 14/500\n",
            "125/125 [==============================] - 10s 73ms/step - loss: 0.9454\n",
            "Epoch 15/500\n",
            "125/125 [==============================] - 10s 73ms/step - loss: 0.9229\n",
            "Epoch 16/500\n",
            "125/125 [==============================] - 10s 73ms/step - loss: 0.9033\n",
            "Epoch 17/500\n",
            "125/125 [==============================] - 10s 73ms/step - loss: 0.8804\n",
            "Epoch 18/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.8619\n",
            "Epoch 19/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.8418\n",
            "Epoch 20/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.8172\n",
            "Epoch 21/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.7952\n",
            "Epoch 22/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.7727\n",
            "Epoch 23/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.7501\n",
            "Epoch 24/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.7275\n",
            "Epoch 25/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.7068\n",
            "Epoch 26/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.6853\n",
            "Epoch 27/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.6614\n",
            "Epoch 28/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.6417\n",
            "Epoch 29/500\n",
            "125/125 [==============================] - 10s 74ms/step - loss: 0.6225\n",
            "Epoch 30/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.6041\n",
            "Epoch 31/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.5860\n",
            "Epoch 32/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.5686\n",
            "Epoch 33/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.5542\n",
            "Epoch 34/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.5340\n",
            "Epoch 35/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.5209\n",
            "Epoch 36/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.5037\n",
            "Epoch 37/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4944\n",
            "Epoch 38/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4796\n",
            "Epoch 39/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.4709\n",
            "Epoch 40/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4632\n",
            "Epoch 41/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4535\n",
            "Epoch 42/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4442\n",
            "Epoch 43/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4366\n",
            "Epoch 44/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4287\n",
            "Epoch 45/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.4210\n",
            "Epoch 46/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4133\n",
            "Epoch 47/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4097\n",
            "Epoch 48/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.4059\n",
            "Epoch 49/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3976\n",
            "Epoch 50/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3946\n",
            "Epoch 51/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3890\n",
            "Epoch 52/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3853\n",
            "Epoch 53/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3791\n",
            "Epoch 54/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3761\n",
            "Epoch 55/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3747\n",
            "Epoch 56/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3713\n",
            "Epoch 57/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3677\n",
            "Epoch 58/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3646\n",
            "Epoch 59/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3609\n",
            "Epoch 60/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3595\n",
            "Epoch 61/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3575\n",
            "Epoch 62/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3561\n",
            "Epoch 63/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3526\n",
            "Epoch 64/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3461\n",
            "Epoch 65/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3452\n",
            "Epoch 66/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3458\n",
            "Epoch 67/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3435\n",
            "Epoch 68/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3402\n",
            "Epoch 69/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3362\n",
            "Epoch 70/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3372\n",
            "Epoch 71/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3312\n",
            "Epoch 72/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3328\n",
            "Epoch 73/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3306\n",
            "Epoch 74/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3237\n",
            "Epoch 75/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3239\n",
            "Epoch 76/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3236\n",
            "Epoch 77/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3218\n",
            "Epoch 78/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3221\n",
            "Epoch 79/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3215\n",
            "Epoch 80/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3165\n",
            "Epoch 81/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3155\n",
            "Epoch 82/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3136\n",
            "Epoch 83/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3121\n",
            "Epoch 84/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3080\n",
            "Epoch 85/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3089\n",
            "Epoch 86/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3057\n",
            "Epoch 87/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3084\n",
            "Epoch 88/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3048\n",
            "Epoch 89/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3029\n",
            "Epoch 90/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.3029\n",
            "Epoch 91/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3019\n",
            "Epoch 92/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3005\n",
            "Epoch 93/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.3004\n",
            "Epoch 94/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2998\n",
            "Epoch 95/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2976\n",
            "Epoch 96/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.2941\n",
            "Epoch 97/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2982\n",
            "Epoch 98/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2941\n",
            "Epoch 99/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2919\n",
            "Epoch 100/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2912\n",
            "Epoch 101/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2921\n",
            "Epoch 102/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2917\n",
            "Epoch 103/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.2887\n",
            "Epoch 104/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2871\n",
            "Epoch 105/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2870\n",
            "Epoch 106/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2866\n",
            "Epoch 107/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2848\n",
            "Epoch 108/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2868\n",
            "Epoch 109/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.2818\n",
            "Epoch 110/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2859\n",
            "Epoch 111/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2830\n",
            "Epoch 112/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2837\n",
            "Epoch 113/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2836\n",
            "Epoch 114/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2840\n",
            "Epoch 115/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2783\n",
            "Epoch 116/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.2782\n",
            "Epoch 117/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2750\n",
            "Epoch 118/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2777\n",
            "Epoch 119/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2790\n",
            "Epoch 120/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2754\n",
            "Epoch 121/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2723\n",
            "Epoch 122/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.2774\n",
            "Epoch 123/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2757\n",
            "Epoch 124/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2709\n",
            "Epoch 125/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2686\n",
            "Epoch 126/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2713\n",
            "Epoch 127/500\n",
            "125/125 [==============================] - 10s 75ms/step - loss: 0.2758\n",
            "Epoch 128/500\n",
            "125/125 [==============================] - 10s 76ms/step - loss: 0.2746\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}