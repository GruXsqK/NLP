{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Lesson_4_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVlndSqJ_v7f"
      },
      "source": [
        "__В блокноте text_classification.ipynb разобрали анализ тональности используя полносвязанную сеть сделать на тех же данных__\n",
        "\n",
        "__1. tf-idf/count vectorizer + логистическую регрессию__\n",
        "\n",
        "__2. Обучить вашу архитектуре сети возможно туже что и была на занятии проверить:__\n",
        "\n",
        "__•\tвзять предобученный эмбединг(к примеру word2vec) и загрузить в слой Embedding__\n",
        "\n",
        "__•\tвзять слой Embedding без предобученных весов__\n",
        "\n",
        "__Сравнить все подходы в том числе и полносвязанную сеть что лучше отработало__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzuCKIQc_v7p"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import linear_model, metrics\n",
        "\n",
        "from tensorflow.keras import layers, Model, Sequential, losses\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, \\\n",
        "    Flatten, GlobalAveragePooling1D, Reshape, Dropout"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni18364S_v7t",
        "outputId": "a6c545c2-3f50-4054-ec6e-dc73429ce459"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FKTGnFt_v7u"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3sX5wVmw_v7v"
      },
      "source": [
        "def get_df_from_files(directory, labels_dirs):\n",
        "    \n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for i, tag in enumerate(labels_dirs):\n",
        "        path_tag = os.path.join(directory, tag)\n",
        "        \n",
        "        for name in tqdm(os.listdir(path_tag)):\n",
        "            file = os.path.join(path_tag, name)\n",
        "            with open(file, 'r', encoding='utf-8') as t:\n",
        "                texts.append(t.read())\n",
        "                labels.append(i)\n",
        "\n",
        "    return pd.DataFrame({'text': texts, 'label': labels}).sample(frac=1).reset_index(drop=True) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqyO7p6J_v7v",
        "outputId": "bc47f553-d9db-449d-982f-b6587173a639"
      },
      "source": [
        "# neg=1, pos=0\n",
        "labels_dirs = ['neg', 'pos']\n",
        "train = get_df_from_files(train_dir, labels_dirs)\n",
        "test = get_df_from_files(test_dir, labels_dirs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12500/12500 [00:00<00:00, 36327.88it/s]\n",
            "100%|██████████| 12500/12500 [00:00<00:00, 37088.88it/s]\n",
            "100%|██████████| 12500/12500 [00:00<00:00, 37396.33it/s]\n",
            "100%|██████████| 12500/12500 [00:00<00:00, 37014.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9dWamQKR_v7w",
        "outputId": "7aeead13-40ee-4fb5-8f81-9231c2be3ba8"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I still find it difficult to comprehend that a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To be as honest as I possibly can, The Devil's...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An old family story told to two young girls by...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ulises is a literature teacher that arrives to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I caught this Cuban film at at an arthouse fil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  I still find it difficult to comprehend that a...      0\n",
              "1  To be as honest as I possibly can, The Devil's...      0\n",
              "2  An old family story told to two young girls by...      1\n",
              "3  Ulises is a literature teacher that arrives to...      0\n",
              "4  I caught this Cuban film at at an arthouse fil...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e9a_DGZAj5d",
        "outputId": "23c08ad0-098e-4fb6-8702-c4561f253a34"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr7eJ_1a_v7w"
      },
      "source": [
        "def removeApostrophe(review):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", review)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", review)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", review)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", review)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", review)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", review)\n",
        "    return phrase\n",
        "\n",
        "def removeApostropheFixed(review):\n",
        "    phrase = re.sub(r\"won't\", \"willnot\", review)\n",
        "    phrase = re.sub(r\"can\\'t\", \"cannot\", review)\n",
        "    phrase = re.sub(r\"\\snot\", \"not\", review)\n",
        "    phrase = re.sub(r\"n\\'t\", \"not\", review)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", review)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", review)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", review)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", review)\n",
        "    return phrase\n",
        "\n",
        "def removeSpecialChars(review):\n",
        "     return re.sub('[^a-zA-Z]', ' ', review)\n",
        "    \n",
        "def doCleaningFixed(review):\n",
        "    review = removeApostropheFixed(review)\n",
        "    review = removeSpecialChars(review) \n",
        "    review = review.lower()  \n",
        "    # review = review.split() #Tokenization\n",
        "    # lmtzr = WordNetLemmatizer()\n",
        "    # review = [lmtzr.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    # review = \" \".join(review)    \n",
        "    return review\n",
        "\n",
        "def doTextCleaning(review):\n",
        "    review = removeApostrophe(review)\n",
        "    review = removeSpecialChars(review) \n",
        "    review = review.lower()  \n",
        "    # review = review.split() #Tokenization\n",
        "    # lmtzr = WordNetLemmatizer()\n",
        "    # review = [lmtzr.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    # review = \" \".join(review)    \n",
        "    return review"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5orP0V7_v7y"
      },
      "source": [
        "train['clean_text'] = train['text'].apply(doTextCleaning)\n",
        "test['clean_text'] = test['text'].apply(doTextCleaning)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "5b74VHJk_v7y",
        "outputId": "848d00e6-20f5-420d-a182-ce1835626556"
      },
      "source": [
        "train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I still find it difficult to comprehend that a...</td>\n",
              "      <td>0</td>\n",
              "      <td>i still find it difficult to comprehend that a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To be as honest as I possibly can, The Devil's...</td>\n",
              "      <td>0</td>\n",
              "      <td>to be as honest as i possibly can  the devil s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An old family story told to two young girls by...</td>\n",
              "      <td>1</td>\n",
              "      <td>an old family story told to two young girls by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ulises is a literature teacher that arrives to...</td>\n",
              "      <td>0</td>\n",
              "      <td>ulises is a literature teacher that arrives to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I caught this Cuban film at at an arthouse fil...</td>\n",
              "      <td>0</td>\n",
              "      <td>i caught this cuban film at at an arthouse fil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>The most agile fat guy in martial arts does it...</td>\n",
              "      <td>1</td>\n",
              "      <td>the most agile fat guy in martial arts does it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>The first thing you meet when you study fascis...</td>\n",
              "      <td>1</td>\n",
              "      <td>the first thing you meet when you study fascis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>This is a wonderful film. The non-stop patter ...</td>\n",
              "      <td>1</td>\n",
              "      <td>this is a wonderful film  the non stop patter ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>We've all been there, sitting with some friend...</td>\n",
              "      <td>0</td>\n",
              "      <td>we ve all been there  sitting with some friend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>where would one start a review of the film Sni...</td>\n",
              "      <td>1</td>\n",
              "      <td>where would one start a review of the film sni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...                                         clean_text\n",
              "0      I still find it difficult to comprehend that a...  ...  i still find it difficult to comprehend that a...\n",
              "1      To be as honest as I possibly can, The Devil's...  ...  to be as honest as i possibly can  the devil s...\n",
              "2      An old family story told to two young girls by...  ...  an old family story told to two young girls by...\n",
              "3      Ulises is a literature teacher that arrives to...  ...  ulises is a literature teacher that arrives to...\n",
              "4      I caught this Cuban film at at an arthouse fil...  ...  i caught this cuban film at at an arthouse fil...\n",
              "...                                                  ...  ...                                                ...\n",
              "24995  The most agile fat guy in martial arts does it...  ...  the most agile fat guy in martial arts does it...\n",
              "24996  The first thing you meet when you study fascis...  ...  the first thing you meet when you study fascis...\n",
              "24997  This is a wonderful film. The non-stop patter ...  ...  this is a wonderful film  the non stop patter ...\n",
              "24998  We've all been there, sitting with some friend...  ...  we ve all been there  sitting with some friend...\n",
              "24999  where would one start a review of the film Sni...  ...  where would one start a review of the film sni...\n",
              "\n",
              "[25000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMsFmKdN_v7z"
      },
      "source": [
        "train_x, train_y = train['clean_text'], train['label']\n",
        "test_x, test_y = test['clean_text'], test['label']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSsJVQfu_v7z"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(train['clean_text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xtest_tfidf =  tfidf_vect.transform(test_x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TReaJIKO_v70"
      },
      "source": [
        "lr_model = linear_model.LogisticRegression(random_state=27, max_iter=300, n_jobs=-1)\n",
        "\n",
        "lr_model.fit(xtrain_tfidf, train_y)\n",
        "accuracy_lr = metrics.accuracy_score(lr_model.predict(xtest_tfidf), test_y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKvTEVOn_v70",
        "outputId": "a51892dd-c818-4d82-a440-554dcf5cd507"
      },
      "source": [
        "batch_size = 64\n",
        "seed = 27\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n",
        "\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)\n",
        "\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeqJezoX_v71"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29XV2f_P_v71"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuJGj9nZ_v72"
      },
      "source": [
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWXUKanLDJ-X"
      },
      "source": [
        "def vectorize_text(text, label):\r\n",
        "    text = tf.expand_dims(text, -1)\r\n",
        "    return vectorize_layer(text), label"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7njMlut_v72"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qPxDRb7_v73"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4-Bz2-r_v73"
      },
      "source": [
        "embedding_dim = 64"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDC474AO_v74",
        "outputId": "0bd79c5d-ca47-4473-d960-4fcc38e94ebe"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    Embedding(max_features + 1, embedding_dim),\n",
        "    Dropout(0.2),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dropout(0.2),\n",
        "    Dense(145),\n",
        "    Dense(1)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 145)               9425      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 146       \n",
            "=================================================================\n",
            "Total params: 649,635\n",
            "Trainable params: 649,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDEwu-bu_v74"
      },
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJBRB5e1_v74",
        "outputId": "3b09ebe9-b3c0-402c-d262-0fd845a42ea0"
      },
      "source": [
        "epochs = 20\r\n",
        "history = model.fit(\r\n",
        "    train_ds,\r\n",
        "    validation_data=val_ds,\r\n",
        "    epochs=epochs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.4859 - binary_accuracy: 0.7578 - val_loss: 0.3076 - val_binary_accuracy: 0.8762\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.2580 - binary_accuracy: 0.8975 - val_loss: 0.2848 - val_binary_accuracy: 0.8880\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.1961 - binary_accuracy: 0.9251 - val_loss: 0.3034 - val_binary_accuracy: 0.8830\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.1570 - binary_accuracy: 0.9431 - val_loss: 0.3392 - val_binary_accuracy: 0.8810\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.1277 - binary_accuracy: 0.9553 - val_loss: 0.3905 - val_binary_accuracy: 0.8750\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.1034 - binary_accuracy: 0.9658 - val_loss: 0.4620 - val_binary_accuracy: 0.8694\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.0853 - binary_accuracy: 0.9720 - val_loss: 0.5362 - val_binary_accuracy: 0.8668\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0732 - binary_accuracy: 0.9747 - val_loss: 0.5949 - val_binary_accuracy: 0.8658\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0636 - binary_accuracy: 0.9783 - val_loss: 0.6829 - val_binary_accuracy: 0.8642\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0722 - binary_accuracy: 0.9733 - val_loss: 0.7021 - val_binary_accuracy: 0.8618\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0796 - binary_accuracy: 0.9692 - val_loss: 0.7256 - val_binary_accuracy: 0.8562\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0718 - binary_accuracy: 0.9737 - val_loss: 0.7143 - val_binary_accuracy: 0.8604\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0703 - binary_accuracy: 0.9728 - val_loss: 0.7275 - val_binary_accuracy: 0.8588\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.0614 - binary_accuracy: 0.9771 - val_loss: 0.7649 - val_binary_accuracy: 0.8600\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.0490 - binary_accuracy: 0.9820 - val_loss: 0.8411 - val_binary_accuracy: 0.8568\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0351 - binary_accuracy: 0.9883 - val_loss: 0.9267 - val_binary_accuracy: 0.8586\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0299 - binary_accuracy: 0.9894 - val_loss: 1.0575 - val_binary_accuracy: 0.8556\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0282 - binary_accuracy: 0.9900 - val_loss: 1.1909 - val_binary_accuracy: 0.8514\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.0314 - binary_accuracy: 0.9893 - val_loss: 1.1712 - val_binary_accuracy: 0.8562\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 8s 25ms/step - loss: 0.0361 - binary_accuracy: 0.9876 - val_loss: 1.1228 - val_binary_accuracy: 0.8582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoX5bDpz_v75",
        "outputId": "de7222c9-9c4e-4f17-ac74-9f8a4d373a89"
      },
      "source": [
        "loss, accuracy_net = model.evaluate(test_ds)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 10s 25ms/step - loss: 1.3863 - binary_accuracy: 0.8278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8R36ECD_v79",
        "outputId": "0772bba6-0d1d-40a7-a969-ffd3aa28d359"
      },
      "source": [
        "print(f\"Accuracy Logistic Regression = {accuracy_lr}\")\r\n",
        "print(f\"Accuracy NET = {accuracy_net}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Logistic Regression = 0.88272\n",
            "Accuracy NET = 0.8278399705886841\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}