{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Улучшение качества  POS-taggera.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets exist\n",
      "./datasets/ru_syntagrus-ud-train.conllu exist\n",
      "./datasets/ru_syntagrus-ud-dev.conllu exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "path_dir = \"./datasets\"\n",
    "path_train = f\"{path_dir}/ru_syntagrus-ud-train.conllu\"\n",
    "path_dev = f\"{path_dir}/ru_syntagrus-ud-dev.conllu\"\n",
    "\n",
    "os.mkdir(path_dir) if not os.path.exists(path_dir) else print(f\"{path_dir} exist\")\n",
    "    \n",
    "url_train = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu'\n",
    "url_dev = 'https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu'\n",
    "\n",
    "urllib.request.urlretrieve(url_train, path_train) if not os.path.exists(path_train) else print(f\"{path_train} exist\")\n",
    "    \n",
    "urllib.request.urlretrieve(url_dev, path_dev) if not os.path.exists(path_dev) else print(f\"{path_dev} exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty=\"elasticnet\",\n",
    "                        random_state=0, \n",
    "                        max_iter=30, \n",
    "                        solver=\"saga\",\n",
    "                        n_jobs=-1,\n",
    "                        l1_ratio=0.1)\n",
    "\n",
    "lr.fit(X_train, train_enc_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7925976476931891"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lr.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.1. Используя библиотеку Spacy, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? (Учтите, что max_word_limit_spacy для Spacy = 1000000).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[when, father, is, dysfunct, and, is, so, self...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, for, lyft, credit, can, not, use, caus...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, your, majesti]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love yoyou take with yoyou all the time ...</td>\n",
       "      <td>[model, love, yoyou, take, with, yoyou, all, t...</td>\n",
       "      <td>[model, love, yoyou, take, yoyou, time, yoyour]</td>\n",
       "      <td>[model, love, yoyou, take, with, yoyou, all, t...</td>\n",
       "      <td>[model, love, yoyou, take, with, yoyou, all, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, now, motiv]</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "3  model love yoyou take with yoyou all the time ...   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, yoyou, take, with, yoyou, all, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3    [model, love, yoyou, take, yoyou, time, yoyour]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [when, father, is, dysfunct, and, is, so, self...   \n",
       "1  [thank, for, lyft, credit, can, not, use, caus...   \n",
       "2                            [bihday, your, majesti]   \n",
       "3  [model, love, yoyou, take, with, yoyou, all, t...   \n",
       "4                   [factsguid, societi, now, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [when, father, is, dysfunctional, and, is, so,...  \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...  \n",
       "2                            [bihday, your, majesty]  \n",
       "3  [model, love, yoyou, take, with, yoyou, all, t...  \n",
       "4             [factsguide, society, now, motivation]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = pd.read_pickle(\"data/processed_tweets.pkl\")\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_limit = 1000000\n",
    "\n",
    "clean_tweets = ''.join(combine_df['clean_tweet'])[:max_word_limit]\n",
    "article = nlp(clean_tweets)\n",
    "# displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DATE', 2453),\n",
       " ('PERSON', 2200),\n",
       " ('GPE', 1286),\n",
       " ('ORG', 1049),\n",
       " ('TIME', 482),\n",
       " ('NORP', 467),\n",
       " ('CARDINAL', 279),\n",
       " ('ORDINAL', 171),\n",
       " ('FAC', 122),\n",
       " ('LOC', 92),\n",
       " ('EVENT', 49),\n",
       " ('PRODUCT', 43),\n",
       " ('LANGUAGE', 15),\n",
       " ('WORK_OF_ART', 13),\n",
       " ('QUANTITY', 6),\n",
       " ('LAW', 2),\n",
       " ('MONEY', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_top = 20\n",
    "\n",
    "Counter([token.label_ for token in article.ents]).most_common(n_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2. С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hea', 33),\n",
       " ('hu', 23),\n",
       " ('blur sun', 13),\n",
       " ('hillary', 10),\n",
       " ('christina grimmie', 10),\n",
       " ('bihday', 9),\n",
       " ('karen iqbal', 7),\n",
       " ('blog', 6),\n",
       " ('punjab', 6),\n",
       " ('dayam', 5),\n",
       " ('michelle', 5),\n",
       " ('possibility anne hathaway', 5),\n",
       " ('youam', 5),\n",
       " ('anton yelchin', 5),\n",
       " ('mm', 4),\n",
       " ('luis suarez', 4),\n",
       " ('don', 4),\n",
       " ('carl paladino', 4),\n",
       " ('jo cox', 4),\n",
       " ('spos', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'PERSON'\n",
    "\n",
    "Counter(token.text for token in article.ents if token.label_==label).most_common(n_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('app', 26),\n",
       " ('gop', 15),\n",
       " ('islam', 15),\n",
       " ('usd', 11),\n",
       " ('bong bing', 10),\n",
       " ('nba', 9),\n",
       " ('fed', 9),\n",
       " ('social analytics', 8),\n",
       " ('usa', 7),\n",
       " ('house', 7),\n",
       " ('congress', 6),\n",
       " ('sma', 6),\n",
       " ('euro', 6),\n",
       " ('eu', 6),\n",
       " ('mac', 6),\n",
       " ('bogota colombia', 6),\n",
       " ('amazon', 4),\n",
       " ('cnn', 4),\n",
       " ('un', 4),\n",
       " ('fbi', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'ORG'\n",
    "\n",
    "Counter(token.text for token in article.ents if token.label_==label).most_common(n_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.3. Повторим шаги из заданий 1.1. и 1.2., используя библиотеку nltk.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(word.istitle() for word in clean_tweets.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(clean_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in tree if hasattr(chunk, 'label')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 41736),\n",
       " ('JJ', 19319),\n",
       " ('IN', 14484),\n",
       " ('NNS', 9177),\n",
       " ('RB', 8592),\n",
       " ('VB', 8287),\n",
       " ('VBP', 7915),\n",
       " ('DT', 7181),\n",
       " ('PRP', 7151),\n",
       " ('VBZ', 5464),\n",
       " ('VBG', 4683),\n",
       " ('TO', 4373),\n",
       " ('VBD', 3213),\n",
       " ('PRP$', 3172),\n",
       " ('CC', 2832),\n",
       " ('VBN', 2576),\n",
       " ('MD', 1915),\n",
       " ('WRB', 896),\n",
       " ('WP', 849),\n",
       " ('RP', 828)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(itm[1] for itm in tree).most_common(n_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как nltk чувствителен к регистру, а все твиты в нижнем регистре, то nltk не может выполнить NER обработанного текста. В подобных случаях можно применять регулярные выражения, задавая для каждого класса свой паттерн."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1. Используя библиотеку nltk, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? Для данного задания используем ограничение на количество символов во входном датасете (max_word_limit_spacy = 1000000), чтобы иметь возможность сравнить результаты работы Spacy и nltk. Обратите внимание, что nltk чувствителен к регистру.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2. С помощью nltk выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Какая из библиотек по вашему лучше отработала? Сравните качество полученных most_common NER и количество распознаных NER.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как в связи с особенностью данных сравнить библиотеки не представляется возможным, то можно только предположить, что spacy, в данном случае, лучше, так как является более гибкой и быстрой. Nltk же работает дольше, но должен обладать более высокой точностью, так как использует классификацию  на основе деревьев."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
